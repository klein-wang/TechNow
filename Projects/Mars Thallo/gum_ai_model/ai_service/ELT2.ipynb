{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "296e980b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "63808440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.4 (main, Jul  5 2023, 08:54:11) [Clang 14.0.6 ]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# 获取当前 Python 的版本信息\n",
    "python_version = sys.version\n",
    "print(python_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "7f2ba6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_OT_SPC(curated_parm, curated_spc):\n",
    "    \n",
    "    # test\n",
    "    # curated_parm = parm\n",
    "    # curated_spc = spc\n",
    "    \n",
    "    # Step 1: 处理 spc 数据\n",
    "    spc = curated_spc[['DataTime', 'Item', 'Load', 'Actual']].copy()\n",
    "    spc.rename(columns={'Actual': 'Weight'}, inplace=True)\n",
    "    \n",
    "    # 根据Item的首字母判断是否含有糖\n",
    "    spc['Sugar'] = np.where(spc['Item'].str[0].isin(['D', 'W', 'R']), 'Sugar', 'Sugarfree')\n",
    "\n",
    "    # 需要的列名列表\n",
    "    required_columns = [\n",
    "        \"TS\",\n",
    "        \"SFBMix.plcSFBMix.dbAdditionalParameter.StateFromSheeting.bMachineRunning\",\n",
    "        \"CG_Sheeting.CG_Sheeting.dbHMI.Sheeting.SRV_Gap3rdSizing.rActualPosition_inches\",\n",
    "        \"CG_Sheeting.CG_Sheeting.dbHMI.Sheeting.SRV_Gap2ndSizing.rActualPosition_inches\",\n",
    "        \"CG_Sheeting.CG_Sheeting.dbHMI.Sheeting.SRV_Gap1stSizing.rActualPosition_inches\",\n",
    "        \"CG_Sheeting.CG_Sheeting.dbHMI.Sheeting.SRV_GapFinalSizing.rActualPosition_inches\",\n",
    "        \"CG_Sheeting.CG_Sheeting.dbHMI.Scoring.SRV_CrossScore.rSetpoint_Ratio\",\n",
    "        \"CG_Sheeting.CG_Sheeting.dbHMI.Cooling.Variables.rChillerSetpoint\",\n",
    "        \"CG_Sheeting.CG_Sheeting.dbHMI.Cooling.Variables.rDrum1InletTemp\",\n",
    "        \"CG_Sheeting.CG_Sheeting.dbHMI.Cooling.Variables.rDrum2InletTemp\",\n",
    "        \"CG_Sheeting.CG_Sheeting.Variables.rGumExtruderExitGumTemp\"\n",
    "    ]\n",
    "    \n",
    "    # Step 2: 处理 para 数据\n",
    "    para = curated_parm.copy()\n",
    " #   para.rename(columns={'TS': 'Date'}, inplace=True)\n",
    "    # 行转列\n",
    "    para = para.pivot(index='TS', columns='Tag', values='Value').reset_index()\n",
    "\n",
    "    # 确保所有必需的列都存在，缺失的列用 NA 填充\n",
    "    for col in required_columns:\n",
    "        if col not in para.columns:\n",
    "            para[col] = np.nan\n",
    "\n",
    "    # 选择列并按顺序排列\n",
    "    para = para[required_columns]\n",
    "    para.fillna(method='ffill', inplace=True)\n",
    "\n",
    "    # later delet this: just to fill the NA\n",
    "    para.fillna(method='bfill', inplace=True)\n",
    "    para['TS'] = pd.to_datetime(para['TS']) + timedelta(minutes=1)\n",
    "    # 将 TS 列重命名为 Date\n",
    "    para.rename(columns={'TS': 'DataTime'}, inplace=True)\n",
    "    \n",
    "    # 提取最新一条记录\n",
    "    # latest_dict = para.iloc[-1, 2:].to_dict()\n",
    "    latest_dict = para.iloc[-1].to_dict()\n",
    "\n",
    "    # Step 3: 进行数据合并与计算\n",
    "    # 当在 spc 数据框中找到一个 Date 值时，它会在 para 数据框中查找等于或早于该 Date 值的最近一行进行匹配。\n",
    "    # 因此，如果 spc 中某个 Date 的值在 para 中找不到完全相同的 Date，则会回退到最接近但早于它的那个 Date 进行匹配。\n",
    "    merge = pd.merge_asof(spc.sort_values('DataTime'), \n",
    "                          para.sort_values('DataTime'), \n",
    "                          on='DataTime', \n",
    "                          direction='backward')\n",
    "    \n",
    "    merge['Prev_Weight'] = merge['Weight'].shift(1)\n",
    "    \n",
    "   \n",
    "    # 计算过去 5, 15 和 30 分钟的均值，不包含当前重量\n",
    "    # closed='left'：指定窗口左闭右开，这意味着在计算滚动平均值时，窗口会排除当前记录的值，只考虑当前记录之前的值。\n",
    "    # df[col] 是一个 Series 对象，而不是一个 DataFrame，因此它无法识别 on='DataTime' 选项。\n",
    "    # 在 DataFrame 上调用 rolling，而不是在 Series 上调用。\n",
    "    def calculate_avg_weight(df, minutes_back, col):\n",
    "        if df[col].isna().all():\n",
    "            return df[col]  # 如果全是空值，返回原列\n",
    "        # 使用 DataFrame 而不是 Series 进行 rolling 操作\n",
    "        return df.rolling(f'{minutes_back}T', on='DataTime', closed='left', min_periods=1)[col].mean()\n",
    "\n",
    "    # 示例：对每个列进行检查并计算滚动平均值\n",
    "    merge['Avg_Weight_5min'] = calculate_avg_weight(merge, 5, 'Weight')\n",
    "    merge['Avg_Weight_15min'] = calculate_avg_weight(merge, 15, 'Weight')\n",
    "    merge['Avg_Weight_30min'] = calculate_avg_weight(merge, 30, 'Weight')\n",
    "    \n",
    "    # 仅处理 key_columns 中除了前两个元素以外的所有列\n",
    "    for col in required_columns[2:]:\n",
    "        merge[f'Prev_{col}'] = merge[col].shift(1)\n",
    "    \n",
    "    # 按照指定顺序重新排列列\n",
    "    # merge 数据框中的列会按照以下顺序排列：\n",
    "    # 固定顺序的列（Date, Load, Item, Sugar, Weight, Prev_Weight, Avg_Weight_5min, Avg_Weight_15min, Avg_Weight_30min）。\n",
    "    # 动态生成的列，这些列以原列名和对应的 Prev_ 前缀列名成对排列。\n",
    "    \n",
    "    # merge = merge[['DataTime', 'Load', 'Item', 'Sugar', \n",
    "    #               'Weight', 'Prev_Weight', 'Avg_Weight_5min', 'Avg_Weight_15min', 'Avg_Weight_30min'] + \n",
    "     #             [col for pair in zip(key_columns, [f'Prev_{col}' for col in key_columns]) for col in pair]]\n",
    "    \n",
    "    # 构建固定的列列表\n",
    "    base_columns = ['DataTime', 'Load', 'Item', 'Sugar', \n",
    "                    'Weight', 'Prev_Weight', 'Avg_Weight_5min', 'Avg_Weight_15min', 'Avg_Weight_30min']\n",
    "\n",
    "    # 动态生成要选择的列，确保选择的列存在于 merge 中\n",
    "    dynamic_columns = [col for pair in zip(key_columns, [f'Prev_{col}' for col in key_columns]) \n",
    "                       for col in pair if col in merge.columns]\n",
    "\n",
    "    # 合并固定列和动态生成的列\n",
    "    selected_columns = base_columns + dynamic_columns\n",
    "\n",
    "    # 选择存在的列，不会因缺少某些列而报错\n",
    "    merge = merge[selected_columns]\n",
    "\n",
    "\n",
    "    # 返回结果\n",
    "    return merge, latest_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "6840bf8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge Latest Dict:\n",
      "{'DataTime': Timestamp('2024-07-06 06:20:56'), 'SFBMix.plcSFBMix.dbAdditionalParameter.StateFromSheeting.bMachineRunning': 1.0, 'CG_Sheeting.CG_Sheeting.dbHMI.Sheeting.SRV_Gap3rdSizing.rActualPosition_inches': 0.07, 'CG_Sheeting.CG_Sheeting.dbHMI.Sheeting.SRV_Gap2ndSizing.rActualPosition_inches': 0.055, 'CG_Sheeting.CG_Sheeting.dbHMI.Sheeting.SRV_Gap1stSizing.rActualPosition_inches': 0.041, 'CG_Sheeting.CG_Sheeting.dbHMI.Sheeting.SRV_GapFinalSizing.rActualPosition_inches': nan, 'CG_Sheeting.CG_Sheeting.dbHMI.Scoring.SRV_CrossScore.rSetpoint_Ratio': 118.7, 'CG_Sheeting.CG_Sheeting.dbHMI.Cooling.Variables.rChillerSetpoint': -15.1, 'CG_Sheeting.CG_Sheeting.dbHMI.Cooling.Variables.rDrum1InletTemp': -17.8, 'CG_Sheeting.CG_Sheeting.dbHMI.Cooling.Variables.rDrum2InletTemp': -14.8, 'CG_Sheeting.CG_Sheeting.Variables.rGumExtruderExitGumTemp': -12.9}\n",
      "\n",
      "Process ETL Data Latest Dict:\n",
      "{'DataTime': Timestamp('2024-07-06 06:19:37'), 'Load': 1, 'Item': 'DMPE 绿箭原味薄荷', 'Sugar': 'Sugar', 'Weight': 34.66, 'Prev_Weight': 35.25, 'Avg_Weight_5min': 35.25, 'Avg_Weight_15min': 34.824999999999996, 'Avg_Weight_30min': 34.90181818181818, 'SFBMix.plcSFBMix.dbAdditionalParameter.StateFromSheeting.bMachineRunning': 1.0, 'CG_Sheeting.CG_Sheeting.dbHMI.Sheeting.SRV_Gap3rdSizing.rActualPosition_inches': 0.07, 'Prev_CG_Sheeting.CG_Sheeting.dbHMI.Sheeting.SRV_Gap3rdSizing.rActualPosition_inches': 0.07, 'CG_Sheeting.CG_Sheeting.dbHMI.Sheeting.SRV_Gap2ndSizing.rActualPosition_inches': 0.055, 'Prev_CG_Sheeting.CG_Sheeting.dbHMI.Sheeting.SRV_Gap2ndSizing.rActualPosition_inches': 0.055, 'CG_Sheeting.CG_Sheeting.dbHMI.Sheeting.SRV_Gap1stSizing.rActualPosition_inches': 0.041, 'Prev_CG_Sheeting.CG_Sheeting.dbHMI.Sheeting.SRV_Gap1stSizing.rActualPosition_inches': 0.041, 'CG_Sheeting.CG_Sheeting.dbHMI.Sheeting.SRV_GapFinalSizing.rActualPosition_inches': nan, 'Prev_CG_Sheeting.CG_Sheeting.dbHMI.Sheeting.SRV_GapFinalSizing.rActualPosition_inches': nan, 'CG_Sheeting.CG_Sheeting.dbHMI.Scoring.SRV_CrossScore.rSetpoint_Ratio': 118.7, 'Prev_CG_Sheeting.CG_Sheeting.dbHMI.Scoring.SRV_CrossScore.rSetpoint_Ratio': 105.2, 'CG_Sheeting.CG_Sheeting.dbHMI.Cooling.Variables.rChillerSetpoint': -15.1, 'Prev_CG_Sheeting.CG_Sheeting.dbHMI.Cooling.Variables.rChillerSetpoint': -15.1, 'CG_Sheeting.CG_Sheeting.dbHMI.Cooling.Variables.rDrum1InletTemp': -17.8, 'Prev_CG_Sheeting.CG_Sheeting.dbHMI.Cooling.Variables.rDrum1InletTemp': -17.8, 'CG_Sheeting.CG_Sheeting.dbHMI.Cooling.Variables.rDrum2InletTemp': -14.8, 'Prev_CG_Sheeting.CG_Sheeting.dbHMI.Cooling.Variables.rDrum2InletTemp': -14.8, 'CG_Sheeting.CG_Sheeting.Variables.rGumExtruderExitGumTemp': -12.9, 'Prev_CG_Sheeting.CG_Sheeting.Variables.rGumExtruderExitGumTemp': -12.9}\n",
      "Returned Merge Latest Dict:\n",
      "{'DataTime': Timestamp('2024-07-06 06:20:56'), 'SFBMix.plcSFBMix.dbAdditionalParameter.StateFromSheeting.bMachineRunning': 1.0, 'CG_Sheeting.CG_Sheeting.dbHMI.Sheeting.SRV_Gap3rdSizing.rActualPosition_inches': 0.07, 'CG_Sheeting.CG_Sheeting.dbHMI.Sheeting.SRV_Gap2ndSizing.rActualPosition_inches': 0.055, 'CG_Sheeting.CG_Sheeting.dbHMI.Sheeting.SRV_Gap1stSizing.rActualPosition_inches': 0.041, 'CG_Sheeting.CG_Sheeting.dbHMI.Sheeting.SRV_GapFinalSizing.rActualPosition_inches': nan, 'CG_Sheeting.CG_Sheeting.dbHMI.Scoring.SRV_CrossScore.rSetpoint_Ratio': 118.7, 'CG_Sheeting.CG_Sheeting.dbHMI.Cooling.Variables.rChillerSetpoint': -15.1, 'CG_Sheeting.CG_Sheeting.dbHMI.Cooling.Variables.rDrum1InletTemp': -17.8, 'CG_Sheeting.CG_Sheeting.dbHMI.Cooling.Variables.rDrum2InletTemp': -14.8, 'CG_Sheeting.CG_Sheeting.Variables.rGumExtruderExitGumTemp': -12.9}\n",
      "\n",
      "Returned Process ETL Data Latest Dict:\n",
      "{'DataTime': Timestamp('2024-07-06 06:19:37'), 'Load': 1, 'Item': 'DMPE 绿箭原味薄荷', 'Sugar': 'Sugar', 'Weight': 34.66, 'Prev_Weight': 35.25, 'Avg_Weight_5min': 35.25, 'Avg_Weight_15min': 34.824999999999996, 'Avg_Weight_30min': 34.90181818181818, 'SFBMix.plcSFBMix.dbAdditionalParameter.StateFromSheeting.bMachineRunning': 1.0, 'CG_Sheeting.CG_Sheeting.dbHMI.Sheeting.SRV_Gap3rdSizing.rActualPosition_inches': 0.07, 'Prev_CG_Sheeting.CG_Sheeting.dbHMI.Sheeting.SRV_Gap3rdSizing.rActualPosition_inches': 0.07, 'CG_Sheeting.CG_Sheeting.dbHMI.Sheeting.SRV_Gap2ndSizing.rActualPosition_inches': 0.055, 'Prev_CG_Sheeting.CG_Sheeting.dbHMI.Sheeting.SRV_Gap2ndSizing.rActualPosition_inches': 0.055, 'CG_Sheeting.CG_Sheeting.dbHMI.Sheeting.SRV_Gap1stSizing.rActualPosition_inches': 0.041, 'Prev_CG_Sheeting.CG_Sheeting.dbHMI.Sheeting.SRV_Gap1stSizing.rActualPosition_inches': 0.041, 'CG_Sheeting.CG_Sheeting.dbHMI.Sheeting.SRV_GapFinalSizing.rActualPosition_inches': nan, 'Prev_CG_Sheeting.CG_Sheeting.dbHMI.Sheeting.SRV_GapFinalSizing.rActualPosition_inches': nan, 'CG_Sheeting.CG_Sheeting.dbHMI.Scoring.SRV_CrossScore.rSetpoint_Ratio': 118.7, 'Prev_CG_Sheeting.CG_Sheeting.dbHMI.Scoring.SRV_CrossScore.rSetpoint_Ratio': 105.2, 'CG_Sheeting.CG_Sheeting.dbHMI.Cooling.Variables.rChillerSetpoint': -15.1, 'Prev_CG_Sheeting.CG_Sheeting.dbHMI.Cooling.Variables.rChillerSetpoint': -15.1, 'CG_Sheeting.CG_Sheeting.dbHMI.Cooling.Variables.rDrum1InletTemp': -17.8, 'Prev_CG_Sheeting.CG_Sheeting.dbHMI.Cooling.Variables.rDrum1InletTemp': -17.8, 'CG_Sheeting.CG_Sheeting.dbHMI.Cooling.Variables.rDrum2InletTemp': -14.8, 'Prev_CG_Sheeting.CG_Sheeting.dbHMI.Cooling.Variables.rDrum2InletTemp': -14.8, 'CG_Sheeting.CG_Sheeting.Variables.rGumExtruderExitGumTemp': -12.9, 'Prev_CG_Sheeting.CG_Sheeting.Variables.rGumExtruderExitGumTemp': -12.9}\n"
     ]
    }
   ],
   "source": [
    "def process_etl_data(now=None,\n",
    "                     parm_file=\"curated_parm.csv\",\n",
    "                     spc_file=\"curated_spc.csv\",\n",
    "                     prev_merge_file=\"merge.csv\",\n",
    "                     merge_OT_SPC=None,\n",
    "                     output_file=\"merge_final_py2.csv\"):\n",
    "    \n",
    "    # Step 1: 读取当前ETL生成的15秒的数据 + 上一个合并文件\n",
    "    parm = pd.read_csv(parm_file)\n",
    "    spc = pd.read_csv(spc_file)\n",
    "    prev_merge = pd.read_csv(prev_merge_file)\n",
    "    \n",
    "    # 转化时间格式为没有时区信息的 pandas.Timestamp 对象\n",
    "    parm['TS'] = pd.to_datetime(parm['TS'], format=\"%Y-%m-%d %H:%M:%S\", utc=True).dt.tz_localize(None)\n",
    "    spc['DataTime'] = pd.to_datetime(spc['DataTime'], format=\"%Y-%m-%d %H:%M:%S\", utc=True).dt.tz_localize(None)\n",
    "    prev_merge['DataTime'] = pd.to_datetime(prev_merge['DataTime'], format=\"%Y-%m-%d %H:%M:%S\", utc=True).dt.tz_localize(None)\n",
    "    \n",
    "    # 删除 parm 中 TS 列为空值的行\n",
    "    parm = parm.dropna(subset=['TS'])\n",
    "\n",
    "    # Step 2: 直接进行合并，获取 merge 和 merge_latest_dict\n",
    "    merge_new, merge_latest_dict = merge_OT_SPC(parm, spc)\n",
    "    \n",
    "    # Step 3: 筛选数据，去掉既不是最近30分钟又不是最近10次的数据\n",
    "    recent_30min = now - timedelta(minutes=30)\n",
    "\n",
    "    # 保留最近30分钟的数据\n",
    "    merge_recent_30min = merge_new[merge_new['DataTime'] >= recent_30min]\n",
    "    \n",
    "    # 保留最近10次的记录\n",
    "    merge_recent_10 = merge_new.tail(10)\n",
    "    \n",
    "    # 选择两者中的较大集合：去掉既不是最近30分钟又不是最近10次的数据作为当前15秒的merge结果\n",
    "    if len(merge_recent_30min) >= len(merge_recent_10):\n",
    "        merge_final = merge_recent_30min\n",
    "    else:\n",
    "        merge_final = merge_recent_10\n",
    "    \n",
    "    # 保存 merge_final 到 output_file\n",
    "    merge_final.to_csv(output_file, index=False)\n",
    "    \n",
    "    # Step 5: 获取最新一条数据（离当前时间最近的一条）\n",
    "    latest_data = merge_final.sort_values(by='DataTime', ascending=False).iloc[0]\n",
    "    \n",
    "    # 生成 dictionary {列名：当前值}\n",
    "    latest_dict = latest_data.to_dict()\n",
    "    \n",
    "    # 输出两个latest_dict进行对比\n",
    "    print(\"Merge Latest Dict:\")\n",
    "    print(merge_latest_dict)\n",
    "    \n",
    "    print(\"\\nProcess ETL Data Latest Dict:\")\n",
    "    print(latest_dict)\n",
    "    \n",
    "    # 返回 merge_latest_dict 和 latest_dict\n",
    "    return merge_latest_dict, latest_dict\n",
    "\n",
    "# 运行示例\n",
    "merge_latest_dict, latest_dict = process_etl_data(now=pd.to_datetime(\"2024-07-06 06:20:00\", format=\"%Y-%m-%d %H:%M:%S\").tz_localize(None),\n",
    "                                                  parm_file=\"curated_parm.csv\",\n",
    "                                                  spc_file=\"curated_spc.csv\",\n",
    "                                                  prev_merge_file=\"merge.csv\",\n",
    "                                                  merge_OT_SPC=merge_OT_SPC,\n",
    "                                                  output_file=\"merge_final_py2.csv\")\n",
    "\n",
    "print(\"Returned Merge Latest Dict:\")\n",
    "print(merge_latest_dict)\n",
    "\n",
    "print(\"\\nReturned Process ETL Data Latest Dict:\")\n",
    "print(latest_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d13f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original\n",
    "def process_etl_data(now=None,\n",
    "                     parm_file=\"curated_parm.csv\",\n",
    "                     spc_file=\"curated_spc.csv\",\n",
    "                     prev_merge_file=\"merge.csv\",\n",
    "                     merge_OT_SPC=None,\n",
    "                     output_file=\"merge_final_py2.csv\"):\n",
    "    \n",
    "    # Step 1: 读取当前ETL生成的15秒的数据 + 上一个合并文件\n",
    "    parm = pd.read_csv(parm_file)\n",
    "    spc = pd.read_csv(spc_file)\n",
    "    prev_merge = pd.read_csv(prev_merge_file)\n",
    "    \n",
    "    # 转化时间格式为没有时区信息的 pandas.Timestamp 对象\n",
    "    parm['TS'] = pd.to_datetime(parm['TS'], format=\"%Y-%m-%d %H:%M:%S\", utc=True).dt.tz_localize(None)\n",
    "    spc['DataTime'] = pd.to_datetime(spc['DataTime'], format=\"%Y-%m-%d %H:%M:%S\", utc=True).dt.tz_localize(None)\n",
    "    prev_merge['DataTime'] = pd.to_datetime(prev_merge['DataTime'], format=\"%Y-%m-%d %H:%M:%S\", utc=True).dt.tz_localize(None)\n",
    "    \n",
    "    # 删除 parm 中 TS 列为空值的行\n",
    "    parm = parm.dropna(subset=['TS'])\n",
    "\n",
    "    # Step 2: 直接进行合并，获取 merge 和 latest_dict\n",
    "    merge_new, merge_latest_dict = merge_OT_SPC(parm, spc)\n",
    "    \n",
    "    # Step 3: 筛选数据，去掉既不是最近30分钟又不是最近10次的数据\n",
    "    recent_30min = now - timedelta(minutes=30)\n",
    "\n",
    "    # 保留最近30分钟的数据\n",
    "    merge_recent_30min = merge_new[merge_new['DataTime'] >= recent_30min]\n",
    "    \n",
    "    # 保留最近10次的记录\n",
    "    merge_recent_10 = merge_new.tail(10)\n",
    "    \n",
    "    # 选择两者中的较大集合：去掉既不是最近30分钟又不是最近10次的数据作为当前15秒的merge结果\n",
    "    if len(merge_recent_30min) >= len(merge_recent_10):\n",
    "        merge_final = merge_recent_30min\n",
    "    else:\n",
    "        merge_final = merge_recent_10\n",
    "    \n",
    "    # 保存 merge_final 到 output_file\n",
    "    merge_final.to_csv(output_file, index=False)\n",
    "    \n",
    "    # Step 5: 获取最新一条数据（离当前时间最近的一条）\n",
    "    latest_data = merge_final.sort_values(by='DataTime', ascending=False).iloc[0]\n",
    "    \n",
    "    # 生成 dictionary {列名：当前值}\n",
    "    latest_dict = latest_data.to_dict()\n",
    "    \n",
    "    # 输出两个latest_dict进行对比\n",
    "    print(\"Merge Latest Dict:\")\n",
    "    print(merge_latest_dict)\n",
    "    \n",
    "    print(\"\\nProcess ETL Data Latest Dict:\")\n",
    "    print(latest_dict)\n",
    "    \n",
    "    # 返回最新数据的 dictionary\n",
    "    return latest_dict\n",
    "\n",
    "# 运行示例\n",
    "result = process_etl_data(now=pd.to_datetime(\"2024-07-06 06:20:00\", format=\"%Y-%m-%d %H:%M:%S\").tz_localize(None),\n",
    "                          parm_file=\"curated_parm.csv\",\n",
    "                          spc_file=\"curated_spc.csv\",\n",
    "                          prev_merge_file=\"merge.csv\",\n",
    "                          merge_OT_SPC=merge_OT_SPC,\n",
    "                          output_file=\"merge_final_py2.csv\")\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
